{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join   \n",
    "                           \n",
    "import pandas as pd                                  \n",
    "import dask.dataframe as dd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#b = db.read_text('../../data/normalization/raw_texts/ekonomi/*.txt',encoding='cp1254')\n",
    "#df = dd.read_csv('../../data/normalization/raw_texts/ekonomi/*.txt',encoding='cp1254',sep='\\n',header=None,assume_missing=True)\n",
    "#print(b.compute())\n",
    "#b.\n",
    "#files = glob.glob('../../data/normalization/raw_texts/ekonomi/*.txt')\n",
    "def read_file(path):\n",
    "    files = glob.glob(path)\n",
    "    texts=[]\n",
    "    listToStr=''\n",
    "    for file in files:\n",
    "       # open the file and then call .read() to get the text\n",
    "       with open(file,\"r\",encoding='cp1254') as f:\n",
    "          text = f.read()\n",
    "          texts.append(text)\n",
    "    listToStr =' '.join(map(str, texts))\n",
    "    return listToStr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.DataFrame(texts, columns=[\"text\"])\n",
    "relative_path = '../../data/normalization/raw_texts';\n",
    "economy_category = '/ekonomi'\n",
    "magazine_category = '/magazin'\n",
    "medical_category = '/saglik'\n",
    "sport_category = '/spor'\n",
    "\n",
    "economy_path = relative_path + economy_category + '/*.txt'\n",
    "magazine_path = relative_path + magazine_category + '/*.txt'\n",
    "medical_path = relative_path + medical_category + '/*.txt'\n",
    "sport_path = relative_path + sport_category + '/*.txt'\n",
    "stop_words_path = relative_path + '/stopwords.txt'\n",
    "economy_str = read_file(economy_path)\n",
    "magazine_str = read_file(magazine_path)\n",
    "medical_str = read_file(medical_path)\n",
    "sport_str = read_file(sport_path)\n",
    "stop_words_str = read_file(stop_words_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    from jpype import JClass, JString, getDefaultJVMPath, shutdownJVM, startJVM   \n",
    "    #shutdownJVM()\n",
    "    ZEMBEREK_PATH: str = join('..', '..', 'bin', 'zemberek-full.jar')              \n",
    "    startJVM(                                                                      \n",
    "        getDefaultJVMPath(),                                                       \n",
    "        '-ea',                                                                     \n",
    "        f'-Djava.class.path={ZEMBEREK_PATH}',                                      \n",
    "        convertStrings=False                                                       \n",
    "    )                                                                           \n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word: kutucuğumuz\n",
      "\n",
      "Results:\n",
      "kutu\n",
      "\tStems = kutu, kutucuğ\n",
      "\tLemmas = kutu, kutucuğ\n",
      "kutu\n",
      "\tStems = kutu, kutucuğ\n",
      "\tLemmas = kutu, kutucuğ\n"
     ]
    }
   ],
   "source": [
    "    TurkishMorphology: JClass = JClass('zemberek.morphology.TurkishMorphology')\n",
    "    WordAnalysis: JClass = JClass('zemberek.morphology.analysis.WordAnalysis')\n",
    "\n",
    "    morphology: TurkishMorphology = TurkishMorphology.createWithDefaults()\n",
    "\n",
    "    word: str = 'kutucuğumuz'\n",
    "\n",
    "    print(f'\\nWord: {word}\\n\\nResults:')\n",
    "\n",
    "    results: WordAnalysis = morphology.analyze(JString(word))\n",
    "    \n",
    "    for result in results:\n",
    "        print(\n",
    "            f'{str(result.getStem())}'\n",
    "            f'\\n\\tStems ='\n",
    "            f' {\", \".join([str(result) for result in result.getStems()])}'\n",
    "            f'\\n\\tLemmas ='\n",
    "            f' {\", \".join([str(result) for result in result.getStems()])}'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "    TurkishTokenizer: JClass = JClass('zemberek.tokenization.TurkishTokenizer')    \n",
    "    Token: JClass = JClass('zemberek.tokenization.Token')                          \n",
    "    tokenizer: TurkishTokenizer = TurkishTokenizer.DEFAULT                         \n",
    "                                                                                   \n",
    "                                                                              \n",
    "    tokenizer: TurkishTokenizer = TurkishTokenizer.builder().ignoreTypes(          \n",
    "        Token.Type.Punctuation,                                                    \n",
    "        Token.Type.NewLine,                                                        \n",
    "        Token.Type.SpaceTab,                                                       \n",
    "        Token.Type.Number,\n",
    "        Token.Type.UnknownWord,\n",
    "        Token.Type.Unknown,\n",
    "        Token.Type.URL,\n",
    "        Token.Type.WordAlphanumerical,\n",
    "        Token.Type.WordWithSymbol,\n",
    "        Token.Type.Abbreviation,\n",
    "        Token.Type.AbbreviationWithDots,\n",
    "        Token.Type.Punctuation,\n",
    "        Token.Type.PercentNumeral,\n",
    "        Token.Type.Time,\n",
    "        Token.Type.Date,\n",
    "        Token.Type.URL,\n",
    "        Token.Type.Email,\n",
    "        Token.Type.HashTag,\n",
    "        Token.Type.Mention,\n",
    "        Token.Type.MetaTag,\n",
    "        Token.Type.Emoji,\n",
    "        Token.Type.Emoticon\n",
    "    ).build() \n",
    "    \n",
    "    economy_tokenized_words = list(tokenizer.tokenize(JString(economy_str)))     \n",
    "    magazine_tokenized_words = list(tokenizer.tokenize(JString(magazine_str)))                            \n",
    "    medical_tokenized_words = list(tokenizer.tokenize(JString(medical_str)))                            \n",
    "    sport_tokenized_words = list(tokenizer.tokenize(JString(sport_str)))                            \n",
    "    stop_tokenized_words = list(tokenizer.tokenize(JString(stop_words_str)))                  \n",
    "    \n",
    "    \n",
    "    #print(len(words))                                                                        \n",
    "    #for index,token in enumerate(medical_words):                                   \n",
    "    #    print(index,token)                                                               \n",
    "                                                                                   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wordSet = set(words)\n",
    "#print(wordSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "    TurkishMorphology: JClass = JClass('zemberek.morphology.TurkishMorphology')\n",
    "    WordAnalysis: JClass = JClass('zemberek.morphology.analysis.WordAnalysis')\n",
    "\n",
    "    morphology: TurkishMorphology = TurkishMorphology.createWithDefaults()\n",
    "    def getWordsStems(tokenized_words):   \n",
    "        stems = []\n",
    "        for word in tokenized_words:\n",
    "                results: WordAnalysis = morphology.analyze(JString(word.content))\n",
    "                for result in results:\n",
    "                    stems.append(str(result.getLemmas()[0]))\n",
    "        return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def removeStopWords(orginal_words,stop_words):\n",
    "        words= []\n",
    "        for word in orginal_words:\n",
    "            if (word not in stop_words):\n",
    "                words.append(word)\n",
    "        return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "   def mapToStringList(stop_words_str):\n",
    "        words=[]\n",
    "        for word in stop_words_str:\n",
    "            words.append(str(word.content))\n",
    "        return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#economy_words = getWordsStems(economy_tokenized_words)\n",
    "#print(economy_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "    economy_words = getWordsStems(economy_tokenized_words)\n",
    "    magazine_words = getWordsStems(magazine_tokenized_words)                             \n",
    "    medical_words = getWordsStems(medical_tokenized_words)                             \n",
    "    sport_words = getWordsStems(sport_tokenized_words) \n",
    "   \n",
    "    economy_words = removeStopWords(economy_words, mapToStringList(stop_tokenized_words))\n",
    "    magazine_words = removeStopWords(magazine_words, mapToStringList(stop_tokenized_words))\n",
    "    medical_words = removeStopWords(medical_words, mapToStringList(stop_tokenized_words))\n",
    "    sport_words = removeStopWords(sport_words, mapToStringList(stop_tokenized_words))\n",
    "    corpus_words = economy_words +magazine_words + medical_words +sport_words\n",
    "    corpus = set(economy_words +magazine_words + medical_words +sport_words)\n",
    "    #stop_tokenized_words = getWordsStems(economy_tokenized_words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125255\n",
      "59338\n",
      "93668\n",
      "100582\n",
      "10198\n",
      "378843\n"
     ]
    }
   ],
   "source": [
    "#print(economy_words)\n",
    "print(len(economy_words))\n",
    "print(len(magazine_words))\n",
    "print(len(medical_words))\n",
    "print(len(sport_words))\n",
    "print(len(corpus))\n",
    "print(len(corpus_words))\n",
    "wordDictEconomy = dict.fromkeys(corpus, 0) \n",
    "wordDictMagazine = dict.fromkeys(corpus, 0) \n",
    "wordDictMedical = dict.fromkeys(corpus, 0) \n",
    "wordDictSport = dict.fromkeys(corpus, 0)\n",
    "wordDictCorpus = dict.fromkeys(corpus, 0)\n",
    "#print(wordDictEconomy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in economy_words:\n",
    "    wordDictEconomy[word]+=1\n",
    "    \n",
    "for word in magazine_words:\n",
    "    wordDictMagazine[word]+=1\n",
    "\n",
    "for word in medical_words:\n",
    "    wordDictMedical[word]+=1\n",
    "\n",
    "for word in sport_words:\n",
    "    wordDictSport[word]+=1\n",
    "    \n",
    "for word in corpus_words:\n",
    "    wordDictCorpus[word]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'benimset': 0,\n",
       " 'arkın': 0,\n",
       " 'lambert': 0,\n",
       " 'haberdar': 2,\n",
       " 'tütsüle': 0,\n",
       " 'nostalji': 0,\n",
       " 'araban': 0,\n",
       " 'ekber': 0,\n",
       " 'olağan': 0,\n",
       " 'sevda': 0,\n",
       " 'otobüs': 15,\n",
       " 'desen': 0,\n",
       " 'çıkı': 2,\n",
       " 'hamdi': 0,\n",
       " 'kapalı': 3,\n",
       " 'hummer': 0,\n",
       " 'authority': 0,\n",
       " 'işbir': 14,\n",
       " 'charlie': 0,\n",
       " 'duyu': 4,\n",
       " 'ırak': 27,\n",
       " 'fizik': 0,\n",
       " 'fizyoterapi': 0,\n",
       " 'tütsü': 0,\n",
       " 'kıvrak': 0,\n",
       " 'koca': 0,\n",
       " 'ball': 0,\n",
       " 'hiperaktivite': 0,\n",
       " 'anket': 32,\n",
       " 'siren': 0,\n",
       " 'karavana': 0,\n",
       " 'sancaktar': 0,\n",
       " 'chiba': 0,\n",
       " 'petder': 2,\n",
       " 'birgün': 0,\n",
       " 'sözlü': 0,\n",
       " 'atrofik': 0,\n",
       " 'esquire': 0,\n",
       " 'bryant': 0,\n",
       " 'ağırla': 0,\n",
       " 'kızarık': 0,\n",
       " 'piyon': 0,\n",
       " 'kota': 13,\n",
       " 'ney': 11,\n",
       " 'son': 882,\n",
       " 'arac': 29,\n",
       " 'ajax': 0,\n",
       " 'kurban': 2,\n",
       " 'gmail': 1,\n",
       " 'zurich': 1,\n",
       " 'ateroskleroz': 0,\n",
       " 'peşinen': 1,\n",
       " 'lakerda': 0,\n",
       " 'türban': 1,\n",
       " 'ahize': 1,\n",
       " 'çap': 17,\n",
       " 'pota': 1,\n",
       " 'sorumsuz': 0,\n",
       " 'bölge': 42,\n",
       " 'pergola': 0,\n",
       " 'bale': 0,\n",
       " 'topuz': 0,\n",
       " 'masal': 0,\n",
       " 'tokuş': 0,\n",
       " 'karmakarışık': 1,\n",
       " 'deprem': 1,\n",
       " 'theron': 0,\n",
       " 'getiri': 8,\n",
       " 'oy': 31,\n",
       " 'gensoru': 2,\n",
       " 'kup': 0,\n",
       " 'barı': 0,\n",
       " 'esinle': 0,\n",
       " 'münir': 0,\n",
       " 'efelik': 3,\n",
       " 'baymak': 1,\n",
       " 'detayla': 0,\n",
       " 'takıntı': 0,\n",
       " 'yang': 8,\n",
       " 'doğrula': 5,\n",
       " 'bakımından': 2,\n",
       " 'susa': 1,\n",
       " 'kat': 154,\n",
       " 'memnuniyet': 5,\n",
       " 'guinness': 5,\n",
       " 'mutlu': 2,\n",
       " 'baba': 19,\n",
       " 'yattara': 0,\n",
       " 'cihan': 0,\n",
       " 'alişan': 0,\n",
       " 'magnetic': 0,\n",
       " 'salla': 0,\n",
       " 'fişek': 0,\n",
       " 'türlü': 12,\n",
       " 'kolla': 0,\n",
       " 'erman': 0,\n",
       " 'konfeti': 0,\n",
       " 'bence': 2,\n",
       " 'derinlik': 2,\n",
       " 'bildir': 105,\n",
       " 'abdullah': 6,\n",
       " 'emre': 0,\n",
       " 'kaplan': 9,\n",
       " 'secret': 0,\n",
       " 'revize': 4,\n",
       " 'küçükusta': 0,\n",
       " 'zeynel': 0,\n",
       " 'rakamla': 13,\n",
       " 'ödemiş': 2,\n",
       " 'kurs': 0,\n",
       " 'ayran': 0,\n",
       " 'corriere': 0,\n",
       " 'şemsi': 0,\n",
       " 'vakıfbank': 1,\n",
       " 'akar': 2,\n",
       " 'ortaklaşa': 6,\n",
       " 'berk': 0,\n",
       " 'basketbolsever': 0,\n",
       " 'bindir': 14,\n",
       " 'birleş': 39,\n",
       " 'tekila': 0,\n",
       " 'nazan': 2,\n",
       " 'dostluk': 0,\n",
       " 'montepaschi': 0,\n",
       " 'mum': 9,\n",
       " 'halter': 3,\n",
       " 'disk': 2,\n",
       " 'akmerkez': 0,\n",
       " 'alliance': 0,\n",
       " 'acın': 0,\n",
       " 'araştırma': 28,\n",
       " 'km': 0,\n",
       " 'tavuk': 0,\n",
       " 'havi': 0,\n",
       " 'say': 55,\n",
       " 'menenjit': 0,\n",
       " 'yardımsever': 1,\n",
       " 'keski': 1,\n",
       " 'canay': 0,\n",
       " 'dns': 2,\n",
       " 'gazü': 0,\n",
       " 'bursaspor': 0,\n",
       " 'makedonya': 0,\n",
       " 'koordine': 0,\n",
       " 'marsilya': 0,\n",
       " 'odak': 5,\n",
       " 'konsantre': 0,\n",
       " 'yyü': 0,\n",
       " 'elle': 0,\n",
       " 'kişi': 118,\n",
       " 'sapta': 12,\n",
       " 'nature': 0,\n",
       " 'kastamonu': 0,\n",
       " 'iste': 179,\n",
       " 'bonservis': 0,\n",
       " 'yasla': 1,\n",
       " 'mica': 0,\n",
       " 'karalama': 0,\n",
       " 'jenerasyon': 0,\n",
       " 'makina': 20,\n",
       " 'dakika': 9,\n",
       " 'gürkan': 0,\n",
       " 'varil': 59,\n",
       " 'gülse': 0,\n",
       " 'adem': 0,\n",
       " 'danny': 0,\n",
       " 'batarya': 0,\n",
       " 'cell': 0,\n",
       " 'şefkat': 0,\n",
       " 'kızılötesi': 0,\n",
       " 'mircea': 0,\n",
       " 'ecdat': 1,\n",
       " 'saz': 0,\n",
       " 'rayo': 0,\n",
       " 'aniden': 0,\n",
       " 'dramatik': 4,\n",
       " 'gökçay': 0,\n",
       " 'uçarer': 0,\n",
       " 'gülsu': 0,\n",
       " 'klinik': 6,\n",
       " 'somer': 2,\n",
       " 'anemi': 2,\n",
       " 'ukala': 0,\n",
       " 'sesle': 2,\n",
       " 'ışılda': 0,\n",
       " 'palto': 0,\n",
       " 'yıldırım': 33,\n",
       " 'simone': 0,\n",
       " 'şenkaya': 1,\n",
       " 'safra': 0,\n",
       " 'şartla': 11,\n",
       " 'turhan': 0,\n",
       " 'faik': 0,\n",
       " 'gazino': 0,\n",
       " 'saban': 13,\n",
       " 'mernis': 0,\n",
       " 'kaş': 0,\n",
       " 'sokak': 4,\n",
       " 'velioğlu': 2,\n",
       " 'viski': 0,\n",
       " 'gösterim': 0,\n",
       " 'tate': 0,\n",
       " 'tower': 0,\n",
       " 'ibrani': 0,\n",
       " 'milimetre': 0,\n",
       " 'ıaaf': 0,\n",
       " 'ölçümle': 0,\n",
       " 'derya': 0,\n",
       " 'görsel': 4,\n",
       " 'ceket': 0,\n",
       " 'meksika': 4,\n",
       " 'aşçı': 0,\n",
       " 'spiegel': 0,\n",
       " 'bulaşıcı': 0,\n",
       " 'aybaba': 0,\n",
       " 'gafil': 0,\n",
       " 'unicredito': 2,\n",
       " 'iyon': 0,\n",
       " 'enformasyon': 1,\n",
       " 'masajla': 0,\n",
       " 'gurur': 1,\n",
       " 'giriş': 51,\n",
       " 'engin': 0,\n",
       " 'imparator': 0,\n",
       " 'yapışık': 0,\n",
       " 'sermaye': 38,\n",
       " 'samimi': 3,\n",
       " 'öğreti': 2,\n",
       " 'maça': 1,\n",
       " 'onassis': 0,\n",
       " 'menkul': 12,\n",
       " 'verecek': 41,\n",
       " 'korkunç': 2,\n",
       " 'temizle': 6,\n",
       " 'çapan': 0,\n",
       " 'içkale': 0,\n",
       " 'resmi': 38,\n",
       " 'kolombiya': 0,\n",
       " 'adanaspor': 0,\n",
       " 'adım': 36,\n",
       " 'sağır': 0,\n",
       " 'anderson': 0,\n",
       " 'litvanya': 0,\n",
       " 'kafes': 0,\n",
       " 'yıkan': 0,\n",
       " 'su': 17,\n",
       " 'teletaş': 1,\n",
       " 'harry': 0,\n",
       " 'nick': 0,\n",
       " 'erdemirspor': 0,\n",
       " 'ryan': 0,\n",
       " 'box': 2,\n",
       " 'david': 1,\n",
       " 'tar': 41,\n",
       " 'ahmed': 0,\n",
       " 'soyutla': 1,\n",
       " 'soba': 0,\n",
       " 'taban': 11,\n",
       " 'dışla': 0,\n",
       " 'ısı': 0,\n",
       " 'sıra': 79,\n",
       " 'kayı': 0,\n",
       " 'efor': 0,\n",
       " 'yaşgünü': 0,\n",
       " 'tesettür': 1,\n",
       " 'kınalıada': 0,\n",
       " 'darülbedayi': 0,\n",
       " 'gibisi': 0,\n",
       " 'payandala': 0,\n",
       " 'biç': 1,\n",
       " 'eğitim': 40,\n",
       " 'afer': 0,\n",
       " 'ticari': 76,\n",
       " 'anastasia': 0,\n",
       " 'çayırlı': 1,\n",
       " 'danıştay': 4,\n",
       " 'ceza': 61,\n",
       " 'basamak': 1,\n",
       " 'benzerlik': 2,\n",
       " 'nisan': 4,\n",
       " 'anımsa': 7,\n",
       " 'berkem': 0,\n",
       " 'dişli': 0,\n",
       " 'odabaş': 0,\n",
       " 'diyet': 0,\n",
       " 'wall': 3,\n",
       " 'anlatım': 2,\n",
       " 'bot': 0,\n",
       " 'ilginç': 5,\n",
       " 'engelle': 16,\n",
       " 'tellal': 2,\n",
       " 'ayıl': 0,\n",
       " 'zemberek': 0,\n",
       " 'yanaş': 3,\n",
       " 'pasaport': 2,\n",
       " 'içinde': 98,\n",
       " 'black': 0,\n",
       " 'filipinli': 0,\n",
       " 'ıp': 0,\n",
       " 'nermi': 0,\n",
       " 'vergi': 346,\n",
       " 'zırva': 0,\n",
       " 'pekkan': 0,\n",
       " 'postala': 2,\n",
       " 'kent': 9,\n",
       " 'enquirer': 0,\n",
       " 'seray': 0,\n",
       " 'estetisyen': 0,\n",
       " 'gürültü': 0,\n",
       " 'ganyan': 0,\n",
       " 'petrol': 259,\n",
       " 'natalia': 0,\n",
       " 'ihd': 0,\n",
       " 'ferdi': 0,\n",
       " 'oyum': 0,\n",
       " 'şerif': 4,\n",
       " 'global': 15,\n",
       " 'tutkun': 0,\n",
       " 'algı': 3,\n",
       " 'gölgele': 0,\n",
       " 'ölü': 10,\n",
       " 'anten': 0,\n",
       " 'ada': 15,\n",
       " 'bisiklet': 1,\n",
       " 'evcil': 0,\n",
       " 'protestan': 0,\n",
       " 'tüt': 20,\n",
       " 'salgı': 0,\n",
       " 'air': 1,\n",
       " 'iltihaplan': 0,\n",
       " 'gerekçe': 24,\n",
       " 'om': 0,\n",
       " 'öre': 0,\n",
       " 'ayıkla': 8,\n",
       " 'formula': 0,\n",
       " 'tcdd': 14,\n",
       " 'asaf': 0,\n",
       " 'associated': 0,\n",
       " 'alüminyum': 0,\n",
       " 'aygıt': 0,\n",
       " 'lütfi': 0,\n",
       " 'düğ': 0,\n",
       " 'kontratak': 0,\n",
       " 'safkan': 0,\n",
       " 'uz': 184,\n",
       " 'cebeci': 2,\n",
       " 'laila': 0,\n",
       " 'allah': 1,\n",
       " 'boks': 0,\n",
       " 'köstebek': 1,\n",
       " 'deterjan': 2,\n",
       " 'turmepa': 0,\n",
       " 'eşraf': 0,\n",
       " 'değiştiri': 0,\n",
       " 'yapraklı': 0,\n",
       " 'altı': 113,\n",
       " 'fidye': 1,\n",
       " 'fel': 0,\n",
       " 'şaka': 0,\n",
       " 'rekabet': 50,\n",
       " 'kıska': 1,\n",
       " 'ileti': 2,\n",
       " 'stok': 64,\n",
       " 'sınırsız': 2,\n",
       " 'nas': 1,\n",
       " 'yılan': 2,\n",
       " 'şampanya': 0,\n",
       " 'yaklaş': 28,\n",
       " 'ılık': 0,\n",
       " 'bosna': 1,\n",
       " 'idil': 0,\n",
       " 'kıs': 74,\n",
       " 'gib': 135,\n",
       " 'bağ': 73,\n",
       " 'truva': 0,\n",
       " 'sendikasyon': 2,\n",
       " 'ken': 40,\n",
       " 'toker': 1,\n",
       " 'birsel': 0,\n",
       " 'civar': 84,\n",
       " 'serbest': 54,\n",
       " 'zeta': 0,\n",
       " 'şemsiye': 0,\n",
       " 'hakimiyet': 2,\n",
       " 'standard': 5,\n",
       " 'bitkisel': 0,\n",
       " 'ziya': 0,\n",
       " 'turna': 0,\n",
       " 'alo': 0,\n",
       " 'şenay': 0,\n",
       " 'faktör': 2,\n",
       " 'pauli': 0,\n",
       " 'scientist': 0,\n",
       " 'üzgü': 0,\n",
       " 'kal': 220,\n",
       " 'ekler': 0,\n",
       " 'şöyle': 65,\n",
       " 'voice': 0,\n",
       " 'sürücü': 2,\n",
       " 'kumkuyu': 0,\n",
       " 'terazi': 4,\n",
       " 'kaçakçı': 4,\n",
       " 'kanalıyla': 1,\n",
       " 'amansız': 1,\n",
       " 'ssk': 56,\n",
       " 'izgi': 0,\n",
       " 'simavi': 0,\n",
       " 'avrupa': 51,\n",
       " 'tercüman': 0,\n",
       " 'hari': 3,\n",
       " 'andersson': 0,\n",
       " 'örgütle': 4,\n",
       " 'abd': 108,\n",
       " 'nizamname': 0,\n",
       " 'ayfer': 0,\n",
       " 'ramazan': 4,\n",
       " 'saraçoğlu': 0,\n",
       " 'hayalle': 0,\n",
       " 'miting': 0,\n",
       " 'mat': 0,\n",
       " 'tai': 1,\n",
       " 'kanca': 0,\n",
       " 'tofaş': 0,\n",
       " 'kurmay': 8,\n",
       " 'pazar': 320,\n",
       " 'fikret': 0,\n",
       " 'yap': 1116,\n",
       " 'al': 1287,\n",
       " 'poli': 8,\n",
       " 'samsun': 4,\n",
       " 'koller': 0,\n",
       " 'dişe': 0,\n",
       " 'haraç': 1,\n",
       " 'restoran': 2,\n",
       " 'önünden': 0,\n",
       " 'alsancak': 0,\n",
       " 'faruk': 3,\n",
       " 'şato': 0,\n",
       " 'giz': 4,\n",
       " 'tenis': 0,\n",
       " 'rövanş': 0,\n",
       " 'batı': 27,\n",
       " 'nötr': 0,\n",
       " 'adli': 4,\n",
       " 'tövbe': 0,\n",
       " 'orda': 1,\n",
       " 'özet': 5,\n",
       " 'abone': 50,\n",
       " 'pall': 1,\n",
       " 'kayıp': 41,\n",
       " 'dezavantaj': 5,\n",
       " 'müsabaka': 0,\n",
       " 'panama': 0,\n",
       " 'müfredat': 7,\n",
       " 'netice': 8,\n",
       " 'omur': 0,\n",
       " 'keleş': 0,\n",
       " 'sonuçlandır': 7,\n",
       " 'la': 2,\n",
       " 'biyoteknoloji': 0,\n",
       " 'vıp': 0,\n",
       " 'akım': 4,\n",
       " 'çakır': 0,\n",
       " 'genelde': 1,\n",
       " 'sokullu': 0,\n",
       " 'zaza': 0,\n",
       " 'hasa': 1,\n",
       " 'sıvı': 2,\n",
       " 'kabus': 1,\n",
       " 'pachulia': 0,\n",
       " 'england': 1,\n",
       " 'çalıntı': 0,\n",
       " 'depresyon': 0,\n",
       " 'caddesi': 4,\n",
       " 'yoga': 0,\n",
       " 'binlerce': 5,\n",
       " 'karaya': 0,\n",
       " 'iktidar': 8,\n",
       " 'şakir': 0,\n",
       " 'nda': 1,\n",
       " 'bira': 22,\n",
       " 'soft': 2,\n",
       " 'kalıp': 16,\n",
       " 'nispi': 1,\n",
       " 'nina': 0,\n",
       " 'prestij': 1,\n",
       " 'arıt': 0,\n",
       " 'tl': 10,\n",
       " 'ok': 0,\n",
       " 'santra': 0,\n",
       " 'mariah': 0,\n",
       " 'arıak': 3,\n",
       " 'ucuz': 24,\n",
       " 'mudi': 4,\n",
       " 'mobil': 6,\n",
       " 'ikizler': 2,\n",
       " 'betonarme': 4,\n",
       " 'baron': 1,\n",
       " 'papaz': 2,\n",
       " 'oksijen': 1,\n",
       " 'kapsam': 102,\n",
       " 'referandum': 16,\n",
       " 'mest': 0,\n",
       " 'aras': 280,\n",
       " 'bahreyn': 1,\n",
       " 'otorite': 14,\n",
       " 'kenan': 1,\n",
       " 'sonuncu': 1,\n",
       " 'nou': 0,\n",
       " 'yaprak': 0,\n",
       " 'üzeri': 535,\n",
       " 'h': 2,\n",
       " 'amiral': 0,\n",
       " 'arab': 3,\n",
       " 'salatalık': 0,\n",
       " 'pacino': 0,\n",
       " 'bastır': 4,\n",
       " 'hormon': 0,\n",
       " 'kongre': 4,\n",
       " 'elbir': 1,\n",
       " 'peşpeşe': 0,\n",
       " 'hazırla': 99,\n",
       " 'yeni': 475,\n",
       " 'gen': 2,\n",
       " 'aşırı': 10,\n",
       " 'özcan': 5,\n",
       " 'santimetre': 0,\n",
       " 'ölümcül': 0,\n",
       " 'köyişleri': 1,\n",
       " 'eşofman': 0,\n",
       " 'belirgin': 6,\n",
       " 'hunt': 0,\n",
       " 'reiner': 0,\n",
       " 'yerçekimi': 0,\n",
       " 'arıboğan': 0,\n",
       " 'beceriksiz': 0,\n",
       " 'mahsur': 0,\n",
       " 'gerile': 49,\n",
       " 'pompa': 11,\n",
       " 'hezimet': 0,\n",
       " 'dem': 2,\n",
       " 'listele': 2,\n",
       " 'coach': 0,\n",
       " 'telkoder': 2,\n",
       " 'kara': 87,\n",
       " 'atm': 3,\n",
       " 'ötanazi': 0,\n",
       " 'tramvay': 4,\n",
       " 'mecburi': 0,\n",
       " 'düşkü': 0,\n",
       " 'abu': 1,\n",
       " 'hayyam': 2,\n",
       " 'mohamed': 1,\n",
       " 'mutlaka': 14,\n",
       " 'kaynaklan': 37,\n",
       " 'form': 19,\n",
       " 'komşu': 12,\n",
       " 'cemiyet': 0,\n",
       " 'alicia': 0,\n",
       " 'solunum': 0,\n",
       " 'emilim': 0,\n",
       " 'mucid': 0,\n",
       " 'mantı': 0,\n",
       " 'kayısı': 0,\n",
       " 'gong': 0,\n",
       " 'holding': 44,\n",
       " 'çarpıntı': 0,\n",
       " 'diz': 4,\n",
       " 'zarif': 0,\n",
       " 'sepang': 0,\n",
       " 'yanıl': 34,\n",
       " 'imam': 1,\n",
       " 'inal': 0,\n",
       " 'kısal': 0,\n",
       " 'portföy': 19,\n",
       " 'leonardo': 0,\n",
       " 'fort': 0,\n",
       " 'iskandinav': 0,\n",
       " 'clippers': 0,\n",
       " 'alzheimer': 2,\n",
       " 'dna': 0,\n",
       " 'erdi': 0,\n",
       " 'ekol': 0,\n",
       " 'takı': 3,\n",
       " 'türkel': 1,\n",
       " 'ilişki': 125,\n",
       " 'şura': 0,\n",
       " 'ritchie': 0,\n",
       " 'plak': 1,\n",
       " 'konteyner': 2,\n",
       " 'haf': 51,\n",
       " 'kademe': 3,\n",
       " 'sabıka': 1,\n",
       " 'püskür': 0,\n",
       " 'praktiker': 2,\n",
       " 'minimum': 2,\n",
       " 'anormal': 0,\n",
       " 'beyazperde': 0,\n",
       " 'kop': 1,\n",
       " 'yeniköy': 0,\n",
       " 'pereira': 0,\n",
       " 'aycell': 1,\n",
       " 'edu': 2,\n",
       " 'sayılı': 7,\n",
       " 'finanse': 7,\n",
       " 'etkile': 44,\n",
       " 'kalkan': 3,\n",
       " 'erzik': 0,\n",
       " 'pirinç': 2,\n",
       " 'yamaha': 1,\n",
       " 'metro': 12,\n",
       " 'okuyan': 4,\n",
       " 'yöre': 0,\n",
       " 'muratoğlu': 7,\n",
       " 'bursa': 5,\n",
       " 'azarla': 0,\n",
       " 'müteşebbis': 1,\n",
       " 'git': 139,\n",
       " 'içiçe': 0,\n",
       " 'ravi': 1,\n",
       " 'sonny': 0,\n",
       " 'alıştır': 0,\n",
       " 'detektör': 0,\n",
       " 'esna': 3,\n",
       " 'ras': 0,\n",
       " 'altınsay': 0,\n",
       " 'karmaş': 0,\n",
       " 'vitrin': 0,\n",
       " 'pe': 0,\n",
       " 'daire': 7,\n",
       " 'pitt': 0,\n",
       " 'istikamet': 0,\n",
       " 'saçma': 0,\n",
       " 'berry': 0,\n",
       " 'tutul': 18,\n",
       " 'reyhan': 0,\n",
       " 'ko': 15,\n",
       " 'deflasyon': 1,\n",
       " 'ak': 52,\n",
       " 'kilogram': 1,\n",
       " 'muhabir': 5,\n",
       " 'alibeyoğlu': 0,\n",
       " 'lacoste': 1,\n",
       " 'flaş': 0,\n",
       " 'bahçe': 0,\n",
       " 'mühendis': 11,\n",
       " 'asya': 0,\n",
       " 'değerlen': 76,\n",
       " 'atakol': 0,\n",
       " 'gıda': 15,\n",
       " 'felek': 0,\n",
       " 'otosan': 1,\n",
       " 'deyince': 1,\n",
       " 'bas': 109,\n",
       " 'dokunulmaz': 0,\n",
       " 'ayrıntıla': 0,\n",
       " 'sakatla': 0,\n",
       " 'taht': 0,\n",
       " 'ergenlik': 0,\n",
       " 'estetik': 0,\n",
       " 'penelope': 0,\n",
       " 'fa': 0,\n",
       " 'toplu': 30,\n",
       " 'kız': 5,\n",
       " 'coke': 4,\n",
       " 'kıraç': 4,\n",
       " 'mostar': 1,\n",
       " 'rodi': 0,\n",
       " 'souness': 0,\n",
       " 'saçmalık': 0,\n",
       " 'kürt': 0,\n",
       " 'cathay': 1,\n",
       " 'afet': 0,\n",
       " 'yanlışlıkla': 0,\n",
       " 'yasir': 0,\n",
       " 'böbrek': 1,\n",
       " 'harvey': 0,\n",
       " 'rocks': 0,\n",
       " 'şimşek': 0,\n",
       " 'merih': 2,\n",
       " 'hasta': 28,\n",
       " 'sinyal': 15,\n",
       " 'tos': 0,\n",
       " 'teşekkür': 1,\n",
       " 'uçukla': 0,\n",
       " 'sonuçlan': 27,\n",
       " 'hadji': 0,\n",
       " 'vadi': 0,\n",
       " 'rasim': 0,\n",
       " 'külliye': 0,\n",
       " 'eleştir': 9,\n",
       " 'bölüş': 0,\n",
       " 'denge': 21,\n",
       " 'f': 0,\n",
       " 'konuş': 119,\n",
       " 'miami': 0,\n",
       " 'nermin': 0,\n",
       " 'bulaş': 0,\n",
       " 'ithalat': 104,\n",
       " 'dağınık': 0,\n",
       " 'j': 3,\n",
       " 'mutlak': 16,\n",
       " 'lisan': 8,\n",
       " 'hawke': 0,\n",
       " 'iran': 13,\n",
       " 'tt': 1,\n",
       " 'hayli': 0,\n",
       " 'giray': 2,\n",
       " 'döküman': 4,\n",
       " 'cup': 0,\n",
       " 'yazın': 4,\n",
       " 'furya': 0,\n",
       " 'esra': 0,\n",
       " 'şölen': 0,\n",
       " 'yarala': 1,\n",
       " 'kızkardeş': 0,\n",
       " 'galler': 0,\n",
       " 'içerik': 16,\n",
       " 'ziyaret': 15,\n",
       " 'vestel': 2,\n",
       " 'yüklem': 4,\n",
       " 'zarfında': 0,\n",
       " 'beyonce': 0,\n",
       " 'mücevher': 0,\n",
       " 'kuşak': 1,\n",
       " 'lütfen': 1,\n",
       " 'onayla': 10,\n",
       " 'gonzales': 0,\n",
       " 'güraçar': 0,\n",
       " 'enfiye': 2,\n",
       " 'akpınar': 8,\n",
       " 'telef': 0,\n",
       " 'yıllığına': 0,\n",
       " 'bası': 31,\n",
       " 'ömür': 7,\n",
       " 'ihraç': 46,\n",
       " 'avlan': 4,\n",
       " 'zübeyde': 0,\n",
       " 'tomislav': 0,\n",
       " 'biyoloji': 0,\n",
       " 'erdemir': 0,\n",
       " 'sus': 7,\n",
       " 'çehre': 0,\n",
       " 'off': 0,\n",
       " 'nane': 0,\n",
       " 'çarli': 1,\n",
       " 'susam': 0,\n",
       " 'suistimal': 0,\n",
       " 'pacific': 1,\n",
       " 'sürü': 13,\n",
       " 'otla': 2,\n",
       " 'örtüş': 0,\n",
       " 'stoper': 0,\n",
       " 'kelim': 0,\n",
       " 'lizbon': 0,\n",
       " 'yorumla': 1,\n",
       " 'bayer': 1,\n",
       " 'sıkıl': 4,\n",
       " 'çelik': 3,\n",
       " 'loca': 0,\n",
       " 'gizle': 3,\n",
       " 'çel': 0,\n",
       " 'demek': 14,\n",
       " 'amatör': 0,\n",
       " 'canavar': 0,\n",
       " 'hoşnut': 0,\n",
       " 'eurovision': 0,\n",
       " 'avni': 1,\n",
       " 'fert': 0,\n",
       " 'ihale': 287,\n",
       " 'yaramaz': 1,\n",
       " 'tavır': 6,\n",
       " 'açın': 3,\n",
       " 'hemfikir': 0,\n",
       " 'cisco': 1,\n",
       " 'berna': 0,\n",
       " 'pile': 0,\n",
       " 'döviz': 58,\n",
       " 'inkar': 0,\n",
       " 'çiçekle': 0,\n",
       " 'siirt': 1,\n",
       " 'kökten': 0,\n",
       " 'farket': 0,\n",
       " 'gurup': 0,\n",
       " 'çankaya': 0,\n",
       " 'kul': 204,\n",
       " 'diane': 0,\n",
       " 'kepeklen': 0,\n",
       " 'noel': 4,\n",
       " 'havas': 2,\n",
       " 'kişisel': 1,\n",
       " 'kopuk': 0,\n",
       " 'elde': 43,\n",
       " 'zara': 2,\n",
       " 'katra': 0,\n",
       " 'radikal': 4,\n",
       " 'mimik': 0,\n",
       " 'pudra': 0,\n",
       " 'talip': 6,\n",
       " 'servi': 13,\n",
       " 'maksat': 1,\n",
       " 'esrarengiz': 1,\n",
       " 'güzel': 24,\n",
       " 'film': 5,\n",
       " 'bun': 330,\n",
       " 'gariban': 4,\n",
       " 'miranda': 0,\n",
       " 'düze': 17,\n",
       " 'istem': 0,\n",
       " 'dede': 5,\n",
       " 'mahalle': 0,\n",
       " 'saniye': 9,\n",
       " 'nasıl': 59,\n",
       " 'azalt': 30,\n",
       " 'nedret': 0,\n",
       " 'yönet': 179,\n",
       " 'dağılım': 4,\n",
       " 'teyit': 1,\n",
       " 'rehin': 5,\n",
       " 'çiftçi': 4,\n",
       " 'çavuşoğlu': 0,\n",
       " 'kambala': 0,\n",
       " 'tommy': 0,\n",
       " 'frankfurt': 1,\n",
       " 'azeri': 4,\n",
       " 'mücahit': 1,\n",
       " 'xavier': 0,\n",
       " 'durak': 17,\n",
       " 'kasıt': 0,\n",
       " 'özay': 0,\n",
       " 'müteahhit': 16,\n",
       " 'yalnızca': 8,\n",
       " 'organizatör': 4,\n",
       " 'devral': 0,\n",
       " 'levrek': 0,\n",
       " 'ayniyat': 0,\n",
       " 'tutku': 0,\n",
       " 'oksit': 0,\n",
       " 'fıstık': 0,\n",
       " 'matem': 0,\n",
       " 'sahife': 0,\n",
       " 'çukur': 0,\n",
       " 'tap': 2,\n",
       " 'baki': 3,\n",
       " 'autumn': 0,\n",
       " 'amaral': 0,\n",
       " 'iplik': 2,\n",
       " 'crystal': 0,\n",
       " 'kuray': 1,\n",
       " 'miss': 0,\n",
       " 'merck': 1,\n",
       " 'yardım': 53,\n",
       " 'mayo': 0,\n",
       " 'kuddusi': 0,\n",
       " 'medyatik': 0,\n",
       " 'dtm': 1,\n",
       " 'kanarya': 0,\n",
       " 'karşı': 465,\n",
       " 'kemere': 0,\n",
       " 'bomboş': 1,\n",
       " 'ms': 0,\n",
       " 'mehmetçik': 0,\n",
       " 'hesapla': 15,\n",
       " 'çiloğlu': 0,\n",
       " 'ret': 0,\n",
       " 'yet': 21,\n",
       " 'süper': 3,\n",
       " 'kramp': 0,\n",
       " 'takvim': 11,\n",
       " 'başrol': 0,\n",
       " 'leblebi': 0,\n",
       " 'mahçup': 0,\n",
       " 'şakalaş': 0,\n",
       " 'gururlan': 0,\n",
       " 'md': 0,\n",
       " 'unsur': 62,\n",
       " 'guard': 0,\n",
       " 'kaçın': 14,\n",
       " 'murat': 26,\n",
       " 'sıcacık': 0,\n",
       " 'southern': 0,\n",
       " 'kazık': 4,\n",
       " 'donanım': 2,\n",
       " 'sürül': 14,\n",
       " 'meng': 0,\n",
       " 'sırt': 0,\n",
       " 'pinel': 0,\n",
       " 'korner': 0,\n",
       " 'nobel': 0,\n",
       " 'beyatlı': 0,\n",
       " 'dolly': 0,\n",
       " 'acar': 0,\n",
       " 'basket': 0,\n",
       " 'aslan': 20,\n",
       " 'lima': 6,\n",
       " 'öib': 8,\n",
       " 'etik': 9,\n",
       " 'simple': 0,\n",
       " 'kaşe': 0,\n",
       " 'şiddetlen': 1,\n",
       " 'henry': 0,\n",
       " 'william': 0,\n",
       " 'yanıtla': 17,\n",
       " 'mans': 0,\n",
       " 'lebedev': 1,\n",
       " 'aids': 0,\n",
       " 'demirağ': 0,\n",
       " 'soy': 13,\n",
       " 'milletvekili': 18,\n",
       " 'vere': 27,\n",
       " 'konuşu': 0,\n",
       " 'hayhay': 0,\n",
       " 'bmw': 4,\n",
       " 'rezidans': 0,\n",
       " 'reyting': 0,\n",
       " 'dan': 2,\n",
       " 'karanfil': 0,\n",
       " 'tuncer': 4,\n",
       " 'hüsna': 0,\n",
       " 'itibari': 20,\n",
       " 'gübre': 0,\n",
       " 'concacaf': 0,\n",
       " 'dedeman': 2,\n",
       " 'ışıltı': 0,\n",
       " 'aşka': 0,\n",
       " 'bakıcı': 0,\n",
       " 'tesadüfen': 0,\n",
       " 'tanes': 4,\n",
       " 'kurcala': 0,\n",
       " 'by': 3,\n",
       " 'akademi': 0,\n",
       " 'ımpossible': 0,\n",
       " 'lara': 0,\n",
       " 'naklet': 3,\n",
       " 'okur': 0,\n",
       " 'kös': 0,\n",
       " 'tabaka': 0,\n",
       " 'yargıç': 2,\n",
       " 'çıkıla': 3,\n",
       " 'eylül': 19,\n",
       " 'belek': 0,\n",
       " 'smear': 0,\n",
       " 'emrullah': 0,\n",
       " 'kasko': 1,\n",
       " 'mevki': 0,\n",
       " 'morfoloji': 0,\n",
       " 'küskü': 0,\n",
       " 'garrido': 0,\n",
       " 'müessese': 0,\n",
       " 'mikrobiyoloji': 0,\n",
       " 'seçenek': 5,\n",
       " 'süregel': 4,\n",
       " 'boğa': 2,\n",
       " 'logo': 5,\n",
       " 'oyun': 21,\n",
       " 'tbmm': 3,\n",
       " 'uma': 0,\n",
       " 'igor': 0,\n",
       " 'sali': 0,\n",
       " 'bilinmez': 0,\n",
       " 'detroit': 0,\n",
       " 'kuruluş': 161,\n",
       " 'deri': 12,\n",
       " 'ileri': 39,\n",
       " 'soğu': 4,\n",
       " 'bologna': 0,\n",
       " 'maslak': 2,\n",
       " 'göğen': 0,\n",
       " 'dani': 0,\n",
       " 'koraç': 0,\n",
       " 'incir': 0,\n",
       " 'icra': 22,\n",
       " 'barışık': 0,\n",
       " 'aliye': 0,\n",
       " 'lise': 0,\n",
       " 'racing': 0,\n",
       " 'tübingen': 0,\n",
       " 'rövaşata': 0,\n",
       " 'inisiyatif': 1,\n",
       " 'sarmala': 0,\n",
       " 'hast': 6,\n",
       " 'kürk': 0,\n",
       " 'tellaliye': 1,\n",
       " 'çiftleş': 2,\n",
       " 'armağan': 0,\n",
       " 'zümrüt': 0,\n",
       " 'dozaj': 0,\n",
       " 'man': 0,\n",
       " 'daum': 0,\n",
       " 'rezerv': 34,\n",
       " 'baştürk': 0,\n",
       " 'mevduat': 54,\n",
       " 'stfa': 4,\n",
       " 'kamış': 2,\n",
       " 'slater': 0,\n",
       " 'basketbol': 0,\n",
       " 'aguirre': 0,\n",
       " 'iğne': 0,\n",
       " 'kombinasyon': 0,\n",
       " 'elena': 0,\n",
       " ...}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordDictEconomy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>benimset</th>\n",
       "      <th>arkın</th>\n",
       "      <th>lambert</th>\n",
       "      <th>haberdar</th>\n",
       "      <th>tütsüle</th>\n",
       "      <th>nostalji</th>\n",
       "      <th>araban</th>\n",
       "      <th>ekber</th>\n",
       "      <th>olağan</th>\n",
       "      <th>sevda</th>\n",
       "      <th>...</th>\n",
       "      <th>sonuçsuz</th>\n",
       "      <th>emanet</th>\n",
       "      <th>kefir</th>\n",
       "      <th>şıkel</th>\n",
       "      <th>uzunluk</th>\n",
       "      <th>felix</th>\n",
       "      <th>dalgıç</th>\n",
       "      <th>uydu</th>\n",
       "      <th>barcelona</th>\n",
       "      <th>yılık</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>72</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 10198 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   benimset  arkın  lambert  haberdar  tütsüle  nostalji  araban  ekber  \\\n",
       "0         0      0        0         2        0         0       0      0   \n",
       "1         0      2        1         1        0         1       0      0   \n",
       "2         2      1        0         1        2         0      20      0   \n",
       "3         0      0        0         0        0         3       4      2   \n",
       "4         2      3        1         4        2         4      24      2   \n",
       "\n",
       "   olağan  sevda  ...  sonuçsuz  emanet  kefir  şıkel  uzunluk  felix  dalgıç  \\\n",
       "0       0      0  ...         0       2      0      0        6      0       0   \n",
       "1       0      8  ...         0      10      0      5        0      0       2   \n",
       "2       1      0  ...         1       0      1      0        5      0       0   \n",
       "3       1      0  ...         0       6      0      0        0      1       0   \n",
       "4       2      8  ...         1      18      1      5       11      1       2   \n",
       "\n",
       "   uydu  barcelona  yılık  \n",
       "0     0          0      1  \n",
       "1     2          0      0  \n",
       "2    68          0      0  \n",
       "3     2         23      0  \n",
       "4    72         23      1  \n",
       "\n",
       "[5 rows x 10198 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#pd.DataFrame([wordDictA, wordDictB, wordDictC ])\n",
    "pd.DataFrame([wordDictEconomy,wordDictMagazine,wordDictMedical,wordDictSport,wordDictCorpus ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combination(m,n):\n",
    "    return math.factorial(m)//(math.factorial(n)*math.factorial(m-n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def computeMeaning(wordDictX,wordDictC,bowX,bowC):\n",
    "    meaningDict = {}\n",
    "    L = len(bowC)\n",
    "    B = len(bowX)\n",
    "    for word, count in wordDictX.items():\n",
    "        if count == 0:\n",
    "            meaningDict[word] = (count-1)*math.log10(L/B)\n",
    "        else:\n",
    "            meaningDict[word] = (-1/count)*(math.log10(combination(wordDictC[word], count)))- ((count-1)*math.log10(L/B))\n",
    "    return meaningDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#meaningBowA = computeMeaning(wordDictA, wordDictC,bowA,bowC)\n",
    "#meaningBowB = computeMeaning(wordDictB, wordDictC,bowB,bowC)\n",
    "\n",
    "meaningEconomy = computeMeaning(wordDictEconomy, wordDictCorpus,economy_words,corpus)\n",
    "meaningMagazine = computeMeaning(wordDictMagazine, wordDictCorpus,magazine_words,corpus)\n",
    "meaningMedical = computeMeaning(wordDictMedical, wordDictCorpus,medical_words,corpus)\n",
    "meaningSport = computeMeaning(wordDictSport, wordDictCorpus,sport_words,corpus)\n",
    "##meaningBowB = computeMeaning(wordDictB, wordDictC,bowB,bowC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>benimset</th>\n",
       "      <th>arkın</th>\n",
       "      <th>lambert</th>\n",
       "      <th>haberdar</th>\n",
       "      <th>tütsüle</th>\n",
       "      <th>nostalji</th>\n",
       "      <th>araban</th>\n",
       "      <th>ekber</th>\n",
       "      <th>olağan</th>\n",
       "      <th>sevda</th>\n",
       "      <th>...</th>\n",
       "      <th>sonuçsuz</th>\n",
       "      <th>emanet</th>\n",
       "      <th>kefir</th>\n",
       "      <th>şıkel</th>\n",
       "      <th>uzunluk</th>\n",
       "      <th>felix</th>\n",
       "      <th>dalgıç</th>\n",
       "      <th>uydu</th>\n",
       "      <th>barcelona</th>\n",
       "      <th>yılık</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.089280</td>\n",
       "      <td>1.089280</td>\n",
       "      <td>1.089280</td>\n",
       "      <td>0.700204</td>\n",
       "      <td>1.089280</td>\n",
       "      <td>1.089280</td>\n",
       "      <td>1.089280</td>\n",
       "      <td>1.089280</td>\n",
       "      <td>1.089280</td>\n",
       "      <td>1.089280</td>\n",
       "      <td>...</td>\n",
       "      <td>1.089280</td>\n",
       "      <td>-0.003066</td>\n",
       "      <td>1.089280</td>\n",
       "      <td>1.089280</td>\n",
       "      <td>5.002293</td>\n",
       "      <td>1.089280</td>\n",
       "      <td>1.089280</td>\n",
       "      <td>1.089280</td>\n",
       "      <td>1.089280</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.764818</td>\n",
       "      <td>0.526257</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.602060</td>\n",
       "      <td>0.764818</td>\n",
       "      <td>-0.602060</td>\n",
       "      <td>0.764818</td>\n",
       "      <td>0.764818</td>\n",
       "      <td>0.764818</td>\n",
       "      <td>5.353725</td>\n",
       "      <td>...</td>\n",
       "      <td>0.764818</td>\n",
       "      <td>6.419255</td>\n",
       "      <td>0.764818</td>\n",
       "      <td>3.059272</td>\n",
       "      <td>0.764818</td>\n",
       "      <td>0.764818</td>\n",
       "      <td>0.764818</td>\n",
       "      <td>-0.938963</td>\n",
       "      <td>0.764818</td>\n",
       "      <td>0.764818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.963076</td>\n",
       "      <td>-0.477121</td>\n",
       "      <td>0.963076</td>\n",
       "      <td>-0.602060</td>\n",
       "      <td>0.963076</td>\n",
       "      <td>0.963076</td>\n",
       "      <td>18.097130</td>\n",
       "      <td>0.963076</td>\n",
       "      <td>-0.301030</td>\n",
       "      <td>0.963076</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.963076</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.963076</td>\n",
       "      <td>3.319377</td>\n",
       "      <td>0.963076</td>\n",
       "      <td>0.963076</td>\n",
       "      <td>64.437691</td>\n",
       "      <td>0.963076</td>\n",
       "      <td>0.963076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.994005</td>\n",
       "      <td>0.994005</td>\n",
       "      <td>0.994005</td>\n",
       "      <td>0.994005</td>\n",
       "      <td>0.994005</td>\n",
       "      <td>1.787324</td>\n",
       "      <td>1.975423</td>\n",
       "      <td>0.994005</td>\n",
       "      <td>-0.301030</td>\n",
       "      <td>0.994005</td>\n",
       "      <td>...</td>\n",
       "      <td>0.994005</td>\n",
       "      <td>4.258581</td>\n",
       "      <td>0.994005</td>\n",
       "      <td>0.994005</td>\n",
       "      <td>0.994005</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.994005</td>\n",
       "      <td>-0.709775</td>\n",
       "      <td>21.868116</td>\n",
       "      <td>0.994005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 10198 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   benimset     arkın   lambert  haberdar   tütsüle  nostalji     araban  \\\n",
       "0  1.089280  1.089280  1.089280  0.700204  1.089280  1.089280   1.089280   \n",
       "1  0.764818  0.526257  0.000000 -0.602060  0.764818 -0.602060   0.764818   \n",
       "2  0.963076 -0.477121  0.963076 -0.602060  0.963076  0.963076  18.097130   \n",
       "3  0.994005  0.994005  0.994005  0.994005  0.994005  1.787324   1.975423   \n",
       "\n",
       "      ekber    olağan     sevda  ...  sonuçsuz    emanet     kefir     şıkel  \\\n",
       "0  1.089280  1.089280  1.089280  ...  1.089280 -0.003066  1.089280  1.089280   \n",
       "1  0.764818  0.764818  5.353725  ...  0.764818  6.419255  0.764818  3.059272   \n",
       "2  0.963076 -0.301030  0.963076  ...  0.000000  0.963076  0.000000  0.963076   \n",
       "3  0.994005 -0.301030  0.994005  ...  0.994005  4.258581  0.994005  0.994005   \n",
       "\n",
       "    uzunluk     felix    dalgıç       uydu  barcelona     yılık  \n",
       "0  5.002293  1.089280  1.089280   1.089280   1.089280  0.000000  \n",
       "1  0.764818  0.764818  0.764818  -0.938963   0.764818  0.764818  \n",
       "2  3.319377  0.963076  0.963076  64.437691   0.963076  0.963076  \n",
       "3  0.994005  0.000000  0.994005  -0.709775  21.868116  0.994005  \n",
       "\n",
       "[4 rows x 10198 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pd.DataFrame([meaningBowA, meaningBowB])\n",
    "pd.DataFrame([meaningEconomy, meaningMagazine,meaningMedical,meaningSport])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "koşar       -0.458023\n",
       "mira        -0.458023\n",
       "şeffaf      -0.408458\n",
       "yapın       -0.408458\n",
       "aydan       -0.408458\n",
       "             ...     \n",
       "et         768.565882\n",
       "çok        833.996206\n",
       "al         835.742561\n",
       "ol        1619.926346\n",
       "iç        2021.616919\n",
       "Length: 10198, dtype: float64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([meaningEconomy, meaningMagazine,meaningMedical,meaningSport]).mean(axis=0).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutdownJVM()                                                                  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
